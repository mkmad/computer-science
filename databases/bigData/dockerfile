# Technical requirements:
# ● Apache Spark 2.3.1
# ● SparkSQL
# ● Scala
# ● Java 8
# ● Hadoop 3


# Create a Dockerfile for the Spark master node
FROM ubuntu

# Install dependencies
RUN apt-get update \
 && apt-get install -y locales \
 && apt-get install -y software-properties-common \
 && dpkg-reconfigure -f noninteractive locales \
 && locale-gen C.UTF-8 \
 && /usr/sbin/update-locale LANG=C.UTF-8 \
 && echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen \
 && locale-gen \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*


# Users with other locales should set this in their derivative image
ENV LANG en_US.UTF-8
ENV LANGUAGE en_US:en
ENV LC_ALL en_US.UTF-8


# PYTHON 
RUN apt-get update \
 && apt-get install -y curl unzip \
    python3 python3-setuptools python3-pip python3-venv \
 && ln -s /usr/bin/python3 /usr/bin/python \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*


# Install py4j
# Need to create a virtual environment to install py4j
# Also need to set the virtual environment path to ensure system python is not used
# https://itsfoss.com/externally-managed-environment/#:~:text=Reason%20behind%20the%20'Externally%20Managed%20Environment'%20Error,-Ubuntu%2023.04%2C%20Fedora&text=The%20change%20has%20been%20done,and%20conflicts%20over%20file%20ownership.%22
RUN python3 -m venv .venv/bigdata
RUN .venv/bigdata/bin/pip install py4j
RUN .venv/bigdata/bin/pip install sbt
ENV PATH $PATH:.venv/bigdata/bin:$PATH


# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed
ENV PYTHONHASHSEED 0
ENV PYTHONIOENCODING UTF-8
ENV PIP_DISABLE_PIP_VERSION_CHECK 1


# JAVA 8
RUN apt-get update \
 && apt-get -y upgrade \
 && apt-get install -y openjdk-8-jdk \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*


# SCALA
ENV SCALA_VERSION 2.11.12
ENV SCALA_HOME /usr/scala-$SCALA_VERSION
ENV PATH $PATH:$SCALA_HOME/bin
RUN curl -sL --retry 3 \
  "https://downloads.lightbend.com/scala/$SCALA_VERSION/scala-$SCALA_VERSION.tgz" \
  | gunzip \
  | tar x -C /usr/ \
 && chown -R root:root $SCALA_HOME


# SBT
ENV SBT_VERSION=1.6.2
RUN \
  mkdir /working/ && \
  cd /working/ && \
  curl -L -o sbt-$SBT_VERSION.deb https://repo.scala-sbt.org/scalasbt/debian/sbt-$SBT_VERSION.deb && \
  dpkg -i sbt-$SBT_VERSION.deb && \
  rm sbt-$SBT_VERSION.deb && \
  apt-get update && \
  apt-get install sbt && \
  cd && \
  rm -r /working/ && \
  sbt sbtVersion


# HADOOP
ENV HADOOP_VERSION 3.0.0
ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PATH $PATH:$HADOOP_HOME/bin
RUN curl -sL --retry 3 \
  "http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz" \
  | gunzip \
  | tar -x -C /usr/ \
 && rm -rf $HADOOP_HOME/share/doc \
 && chown -R root:root $HADOOP_HOME


# SPARK
ENV SPARK_VERSION 2.3.1
ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop
ENV SPARK_HOME /usr/spark-${SPARK_VERSION}
ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"
ENV PATH $PATH:${SPARK_HOME}/bin
RUN curl -sL --retry 3 \
  "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz" \
  | gunzip \
  | tar x -C /usr/ \
 && mv /usr/$SPARK_PACKAGE $SPARK_HOME \
 && chown -R root:root $SPARK_HOME


WORKDIR $SPARK_HOME

# SPARK MASTER
CMD ["bin/spark-class", "org.apache.spark.deploy.master.Master"]
